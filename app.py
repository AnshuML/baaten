import streamlit as st
import os
from dotenv import load_dotenv
from datetime import datetime
import langdetect
from rank_bm25 import BM25Okapi
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_google_genai import GoogleGenerativeAI
from langchain.retrievers import BM25Retriever

# Load environment variables
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# Reranking using BM25
def bm25_rerank(question, docs, top_n=5):
    corpus = [doc.page_content for doc in docs]
    tokenized_corpus = [doc.lower().split() for doc in corpus]
    bm25 = BM25Okapi(tokenized_corpus)
    tokenized_query = question.lower().split()
    scores = bm25.get_scores(tokenized_query)
    reranked_docs = [doc for _, doc in sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)]
    return reranked_docs[:top_n]

# Set Streamlit page config
st.set_page_config(page_title="HR Mind", layout="centered", page_icon="üíº")

# Greeting logic
hour = datetime.now().hour
if 5 <= hour < 12:
    greeting = "Good morning!"
elif 12 <= hour < 18:
    greeting = "Good afternoon!"
elif 18 <= hour < 22:
    greeting = "Good evening!"
else:
    greeting = "Good night!"

# Stylish header
st.markdown(f"""
<div style='
    background-color:#102027;
    border-radius:18px;
    padding:30px;
    text-align:center;
    margin: 20px auto;
    max-width:600px;
'>
    <h1 style='color:white;'>HR Mind</h1>
    <div style='color:#f50057; font-weight:700;'>AIPL Group</div>
    <div style='color:#ffeb3b; font-weight:600; margin-top:10px;'>{greeting}</div>
</div>
""", unsafe_allow_html=True)

# Chat history
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# User input
question = st.text_input("", placeholder="üí¨ Ask a question from HR Constitution:", key="user_input")

if question:
    try:
        # Detect language (optional)
        try:
            lang = langdetect.detect(question)
        except:
            lang = "en"

        # Load embeddings
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=GOOGLE_API_KEY
        )

        # Load FAISS vector store
        db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

        # Retrieve top 15 documents from FAISS
        docs = db.similarity_search(question, k=15)

        if not docs:
            st.warning("‚ö†Ô∏è No relevant information found.")
        else:
            # Rerank with BM25
            reranked_docs = bm25_rerank(question, docs, top_n=5)

            # Create BM25Retriever with reranked documents
            tokenizer = lambda text: text.split()
            retriever = BM25Retriever.from_documents(reranked_docs, tokenizer=tokenizer)

            # Build prompt
            prompt_template = """
            Use the following pieces of context to answer the question at the end. 
            If you don't know the answer, just say that you don't know, don't try to make up an answer.

            Context: {context}

            Question: {question}

            Answer:
            """
            PROMPT = PromptTemplate.from_template(prompt_template)

            # Initialize Gemini LLM
            llm = GoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.3, google_api_key=GOOGLE_API_KEY)
            # Build QA chain
            chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                input_key="question",
                return_source_documents=True,
                chain_type_kwargs={"prompt": PROMPT}
            )
        
            chain_type_kwargs={"prompt": PROMPT}
            

            # Get response
            response = chain.invoke({"question": question})

            # Display response
            st.chat_message("user").markdown(question)
            st.chat_message("assistant").markdown(response["result"])

            # Save to chat history
            st.session_state.chat_history.append({"role": "user", "content": question})
            st.session_state.chat_history.append({"role": "assistant", "content": response["result"]})

    except Exception as e:
        st.error(f"‚ùå Error: {e}")